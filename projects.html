<!DOCTYPE html>
<html  >
<head>
  <!-- Site made with Mobirise Website Builder v5.6.0, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v5.6.0, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/mbr-142x111.png" type="image/x-icon">
  <meta name="description" content="">
  
  
  <title>Ben Lipkin - Projects</title>
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/socicon/css/styles.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Jost:100,200,300,400,500,600,700,800,900,100i,200i,300i,400i,500i,600i,700i,800i,900i&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Jost:100,200,300,400,500,600,700,800,900,100i,200i,300i,400i,500i,600i,700i,800i,900i&display=swap"></noscript>
  <link rel="preload" as="style" href="assets/mobirise/css/mbr-additional.css"><link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">
  
  
  
  
</head>
<body>
  
  <section data-bs-version="5.1" class="menu cid-sFCfta6nVU" once="menu" id="menu1-q">
    
    <nav class="navbar navbar-dropdown navbar-fixed-top navbar-expand-lg">
        <div class="container">
            <div class="navbar-brand">
                <span class="navbar-logo">
                    <a href="index.html">
                        <img src="assets/images/mbr-142x111.png" alt="Ben Lipkin" style="height: 3.8rem;">
                    </a>
                </span>
                <span class="navbar-caption-wrap"><a class="navbar-caption text-black text-primary display-7" href="index.html">Ben Lipkin</a></span>
            </div>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbarSupportedContent" data-bs-target="#navbarSupportedContent" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
                <div class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-dropdown nav-right" data-app-modern-menu="true"><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="projects.html">Projects</a></li><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="papers.html">Papers</a></li>
                    <li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="personal.html">Personal</a></li><li class="nav-item"><a class="nav-link link text-black text-primary display-4" href="contact.html">Contact &amp; CV</a></li></ul>
                
                
            </div>
        </div>
    </nav>

</section>

<section data-bs-version="5.1" class="content1 cid-sFCWe8smrg" id="content1-1r">
    
    <div class="container">
        <div class="row justify-content-center">
            <div class="title col-12 col-md-11">
                <h3 class="mbr-section-title mbr-fonts-style align-center mb-4 display-2"><strong>Selected Projects</strong></h3>
                <h4 class="mbr-section-subtitle align-center mbr-fonts-style mb-4 display-7"><strong>CoCoSci - Joshua Tenenbaum - MIT<br></strong><br><strong><em>Evaluating uncertainty in probabilistic semantic parsing.</em></strong><br>While large language models (LLMs) have provided extremely impressive models of linguistic competence, such systems have also struggled considerably on tasks that require logical, relational, and algorithmic reasoning. One promising approach to address this limitation has been to leverage the capacity of LLMs as semantic parsers, to extract logical predicates and programs from natural language (NL) text, and return such representations as executable code in a general-purpose (e.g., Python), probabilistic (e.g., Church), or domain-specific (e.g., SQL) programming language (PL). This allows actual execution to be off-loaded to a symbolic interpreter or inference engine, provided the "translation" from text to code is accurate. While such approaches thus far have driven huge improvements relative to using an LLM end-to-end, they often reflect the simplifying assumption of a bijective (one-to-one) mapping between statements in NL and programs in a PL. In particular, NL is inherently filled with underspecification, vagueness, and ambiguity, and as such, often requires consideration of a probability distribution over possible logical interpretations in order to faithfully represent the intended semantics. As the first step towards addressing this problem, I am exploring several scenarios where NL is indeed known to be under-defined, and evaluating whether the probability distribution over possible interpretations considered by sampling from LLMs vs. evaluating the crowd-sourced judgements of human participants, converge or diverge in systematic ways. Future work will consider whether maintaining such distributions, and updating them dynamically as more information becomes available, improves multi-step task performance.<br><br>Manuscripts:<br>1) Lipkin B, Wong C, Grand G, and Tenenbaum J. (Under Review). Exploring Uncertainty in Probabilistic Semantic Parsing. <a href="https://github.com/benlipkin/probsem" class="text-primary" target="_blank">Code</a>.<br><strong><br></strong><br><strong>EvLab - Evelina Fedorenko - MIT</strong><strong><br></strong><br><strong><em>'Brain-Score': Dual-stream evaluations of human language and LLMs.</em></strong><br>Why does human language processing look the way it does? To what extent are the inductive biases of human language reflected in artificial language models? This project aims to investigate this dual stream of inference by benchmarking a large set of language models varying in scale, architecture, training corpus, and training objective against human-derived behavioral and neural measurements, to elucidate which language modeling design decisions lead to models that perform more similarly to humans. In this evaluation paradigm, human language benchmarks range from behavioral acceptability judgements, reading-time estimates calculated from surprisal, and even evaluations of derived representational geometry in the human language network and model hidden states. We are exploring a number of questions using this approach, by manipulating models during training along targeted dimensions, and evaluating how these manipulations affect the human-likeness of the resulting models. Some of these manipulations include: limiting the number of tokens seen to approximate human language acquisition, constraining the context window to reflect human working memory constraints, re-structuring training corpora to reflect counterfactual dependency grammars, and leveraging alternative training objectives, including jointly modeling the joint distribution over syntactic tree structures with word sequences. Some current papers from colleagues in this research program include <a href="https://www.pnas.org/doi/10.1073/pnas.2105646118" class="text-primary" target="_blank">Schrimpf et al. (2021)</a> and <a href="https://www.biorxiv.org/content/10.1101/2022.10.04.510681v1" class="text-primary" target="_blank">Hosseini et al. (2022)</a>. More work in this space is currently underway. Check out our <a href="https://github.com/brain-score/language" class="text-primary" target="_blank">GitHub</a> for more info on the Brain-Score project.<br><br><strong><em>Neural representation of computer programs.</em><br></strong>How does the brain represent computer programs? How do artificial systems represent computer programs? Following up on work by <a href="http://evlab.mit.edu/assets/papers/Ivanova_et_al_2020_eLife.pdf" class="text-primary" target="_blank">Ivanova et al. (2020)</a>, who showed that comprehending programming and natural languages utilize different neural resources, we set out to investigate the properties encoded in human and artificial neural representations of computer code. A group of 24 participants were instructed to calculate the output of short python programs during an fMRI session. These programs varied in content (math calculation vs. string manipulation), structure (sequential vs. iterative vs. conditional), and various continuous static (e.g. # of AST nodes) and dynamic (e.g. runtime operations) properties. We extracted voxel pattern responses during program comprehension and problem solving, and probed these representations for relevant properties using a collection of linear decoders. In parallel, we passed these same programs to computational language models of code, ranging from BOW to SOTA transformers trained on large code corpuses, and probed the resulting embeddings for the same code properties. Finally, we investigated affine mappings between the multivariate representations of these programs in functionally localized brain networks and in code models, to evaluate and compare their learned representations, as a function of model architecture and training objective. We found that several distinct brain networks and code models encode specific aspects of code syntax and semantics, and that particular pairs of these systems share linearly decodable information across their learned representations. Notably, at least two computational regimes emerged from the neural signal: one consistent with static string parsing, and one consistent with dynamic execution and simulation. Since current code models are optimized solely for static language modeling objectives, such findings prompt the exploration of new objectives that incorporate simulation and state tracking. Detailed analyses and results are available in the following paper and code base.<br><br>Manuscripts:<br>1) Srikant S*, Lipkin B*, Ivanova A, Fedorenko E, O'Reilly UM. (2022). Convergent Representations of Computer Programs in Human and Artificial Neural Networks. <em>Advances in Neural Information Processing Systems (NeurIPS)</em><span style="font-size: 1.2rem;">.&nbsp;</span><a href="https://openreview.net/pdf?id=AqexjBWRQFx" class="text-primary" target="_blank" style="font-size: 1.2rem;">Paper</a><span style="font-size: 1.2rem;">.&nbsp;</span><a href="https://github.com/benlipkin/braincode" class="text-primary" target="_blank" style="font-size: 1.2rem;">Code</a><span style="font-size: 1.2rem;">. <a href="https://news.mit.edu/2022/your-brain-your-brain-code-1221" class="text-primary" target="_blank">News</a>.</span><div><div><br><em><strong>Characterizing functional brain networks.<br></strong></em>What are the topographies of the brain's core functional networks, e.g. language, multiple demand (MD), theory of mind (ToM), default-mode (DMN), etc.? Under what task contexts do additional regions, e.g. AngG, BTLA, Cerebellum, Hippocampus, engage dynamically with such core networks? Are individual differences in network and fROI-level responses stable and reliable over time? We leverage &gt;100TB data from 1000+ sessions and 100+ experiments collected over 15+ years to functionally localize key brain networks on an individual level. We measure the responses of these networks and supplemental regions to hundreds of conditions to build up detailed response profiles. We then evaluate stability and reliability of responses on both short (between runs within a session) and long (between sessions; spanning up to years) time-scales. Additionally, we aggregate individual network responses to build probabilistic atlases of each network topography, providing a valuable tool for effect localization in future studies. Explore part of this dataset interactively at the <a href="https://evlabwebapps.mit.edu/langatlas" class="text-primary" target="_blank">LanA</a>&nbsp;web application!</div><br><div>Manuscripts:</div><div>1) Lipkin B, Tuckute G, Affourtit J, Small H, Mineroff Z, Kean H, Jouravlev&nbsp;<span style="font-size: 1.2rem;">O, Rakocevic L, Pitchett B, Siegelman M,&nbsp;</span><span style="font-size: 1.2rem;">Hoeflin C, Pongos A, Blank I, </span><span style="font-size: 1.2rem;">Kline M, Ivanova A, Shannon S, Sathe A, Hoffman M, Nieto-Castañón A,&nbsp;</span><span style="font-size: 1.2rem;">and Fedorenko E. (2022). Probabilistic atlas for the language network based&nbsp;</span><span style="font-size: 1.2rem;">on precision fMRI data from &gt;800 individuals. <em>Nature Scientific Data</em></span><span style="font-size: 1.2rem;">. <a href="https://www.nature.com/articles/s41597-022-01645-3" class="text-primary" target="_blank">Paper</a>. <a href="https://osf.io/kzwbh/" class="text-primary" target="_blank">Code</a>. <a href="https://figshare.com/articles/dataset/LanA_Dataset/20425209" class="text-primary" target="_blank">Dataset</a>.</span></div></div><div><div>2) Shain C*, Paunov A*, Chen X*, Lipkin B, and Fedorenko E. (2022). No evidence of theory of mind reasoning in the human language network. <em>Cerebral Cortex.</em>&nbsp;<a href="https://academic.oup.com/cercor/advance-article-abstract/doi/10.1093/cercor/bhac505/6964589" class="text-primary" target="_blank">Paper</a>. <a href="https://osf.io/bzwm8/" class="text-primary" target="_blank">Code</a>.</div></div></h4>
                
            </div>
        </div>
    </div>
</section>

<section data-bs-version="5.1" class="footer3 cid-sFCftaCxsS" once="footers" id="footer3-r">

    

    

    <div class="container">
        <div class="media-container-row align-center mbr-white">
            
            
            <div class="row row-copirayt">
                <p class="mbr-text mb-0 mbr-fonts-style mbr-white align-center display-7">
                    © Ben Lipkin</p>
            </div>
        </div>
    </div>
</section><section style="background-color: #fff; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif; color:#aaa; font-size:12px; padding: 0; align-items: center; display: flex;"><a href="https://mobirise.site/g" style="flex: 1 1; height: 3rem; padding-left: 1rem;"></a><p style="flex: 0 0 auto; margin:0; padding-right:1rem;"><a href="https://mobirise.site/c" style="color:#aaa;">This</a> page was built with Mobirise</p></section><script src="assets/bootstrap/js/bootstrap.bundle.min.js"></script>  <script src="assets/smoothscroll/smooth-scroll.js"></script>  <script src="assets/ytplayer/index.js"></script>  <script src="assets/dropdown/js/navbar-dropdown.js"></script>  <script src="assets/theme/js/script.js"></script>  
  
  
</body>
</html>